{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Global"],"metadata":{"id":"U-u7WV0ROaGp"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import entropy\n","import pandas as pd\n","from scipy.stats import ks_2samp\n","from scipy.spatial.distance import cosine\n","from statsmodels.stats.proportion import proportions_ztest"],"metadata":{"id":"gNaxYRLfu64L","executionInfo":{"status":"ok","timestamp":1723889685444,"user_tz":-180,"elapsed":1761,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# KL"],"metadata":{"id":"WcvT_oSLu73F"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"85nKJ6LOpcq9","executionInfo":{"status":"ok","timestamp":1723889685444,"user_tz":-180,"elapsed":7,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"outputs":[],"source":["# Function to calculate KL divergence between two distributions\n","def calculate_kl_divergence(dist1, dist2):\n","      return entropy(dist1, dist2)\n","\n","def KL(df):\n","  # Drop the id column for aggregation\n","  pattern_data = df.drop(columns=['id', 'class_name'])\n","\n","  kl_results = []\n","  for i in range(10):\n","    # Split the data randomly into two groups (half and half)\n","    random_indices = np.random.permutation(len(pattern_data))\n","    mid_index = len(pattern_data) // 2\n","    group1_indices = random_indices[:mid_index]\n","    group2_indices = random_indices[mid_index:]\n","\n","    group1 = pattern_data.iloc[group1_indices]\n","    group2 = pattern_data.iloc[group2_indices]\n","\n","    # Aggregate pattern counts within each group\n","    group1_aggregated = group1.sum(axis=0)\n","    group2_aggregated = group2.sum(axis=0)\n","    # Normalize the aggregated pattern counts to create valid probability distributions\n","    group1_distribution = group1_aggregated / group1_aggregated.sum()\n","    group2_distribution = group2_aggregated / group2_aggregated.sum()\n","\n","\n","    # Compute KL divergence between the two group-level distributions\n","    kl_results.append(calculate_kl_divergence(group1_distribution, group2_distribution))\n","\n","  return np.round(np.mean(kl_results), 3)"]},{"cell_type":"code","source":["def KL_diversity(group1, group2):\n","  group1 = group1.drop(columns=['id', 'class_name'])\n","  group2 = group2.drop(columns=['id', 'class_name'])\n","  # Aggregate pattern counts within each group\n","  group1_aggregated = group1.sum(axis=0)\n","  group2_aggregated = group2.sum(axis=0)\n","\n","  # Normalize the aggregated pattern counts to create valid probability distributions\n","  group1_distribution = group1_aggregated / group1_aggregated.sum()\n","  group2_distribution = group2_aggregated / group2_aggregated.sum()\n","\n","\n","  # Compute KL divergence between the two group-level distributions\n","  return calculate_kl_divergence(group1_distribution, group2_distribution)"],"metadata":{"id":"SjqEdvPTz8Ub","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":7,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# KS"],"metadata":{"id":"Oejzd7Yn8gBH"}},{"cell_type":"code","source":["def KS(df):\n","  # Drop the id column for aggregation\n","  pattern_data = df.drop(columns=['id', 'class_name'])\n","\n","  ks_results = []\n","  p_val = []\n","  results = {'Fold': [], 'KS Statistic': [], 'P-Value': [], 'Result': []}\n","  for i in range(10):\n","    # Split the data randomly into two groups (half and half)\n","    random_indices = np.random.permutation(len(pattern_data))\n","    mid_index = len(pattern_data) // 2\n","    group1_indices = random_indices[:mid_index]\n","    group2_indices = random_indices[mid_index:]\n","\n","    group1 = pattern_data.iloc[group1_indices]\n","    group2 = pattern_data.iloc[group2_indices]\n","\n","    # Aggregate pattern counts within each group\n","    group1_aggregated = group1.sum(axis=0)\n","    group2_aggregated = group2.sum(axis=0)\n","\n","    ks_statistic, p_value = ks_2samp(group1_aggregated, group2_aggregated)\n","\n","    if p_value > 0.05:\n","      r = 'similar'\n","    else:\n","      r = 'not similar'\n","\n","    results['Fold'].append(i)\n","    results['KS Statistic'].append(ks_statistic)\n","    results['P-Value'].append(p_value)\n","    results['Result'].append(r)\n","\n","  return pd.DataFrame(results)\n","\n"],"metadata":{"id":"nR5-MG7pET4f","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":7,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def KS_test(group1, group2):\n","  group1 = group1.drop(columns=['id', 'class_name'])\n","  group2 = group2.drop(columns=['id', 'class_name'])\n","  # Aggregate pattern counts within each group\n","  group1_aggregated = group1.sum(axis=0)\n","  group2_aggregated = group2.sum(axis=0)\n","\n","  ks_statistic, p_value = ks_2samp(group1_aggregated, group2_aggregated)\n","\n","  # Interpretation\n","  if p_value > 0.05:\n","    r = \"similar\"\n","  else:\n","    r = \"not similar\"\n","\n","  result = {\"KS Statistic\": [ks_statistic], \"P-Value\": [p_value], \"Result\": [r]}\n","  return pd.DataFrame(result)\n"],"metadata":{"id":"LmYuoieZHB-O","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":7,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# COSINE"],"metadata":{"id":"kPBCfCwiIlUC"}},{"cell_type":"code","source":["def COSINE(df):\n","  # Drop the id column for aggregation\n","  pattern_data = df.drop(columns=['id', 'class_name'])\n","\n","  results = {\"Fold\": [], \"Cosine Similarity\": [], \"Result\": []}\n","  for i in range(10):\n","    # Split the data randomly into two groups (half and half)\n","    random_indices = np.random.permutation(len(pattern_data))\n","    mid_index = len(pattern_data) // 2\n","    group1_indices = random_indices[:mid_index]\n","    group2_indices = random_indices[mid_index:]\n","\n","    group1 = pattern_data.iloc[group1_indices]\n","    group2 = pattern_data.iloc[group2_indices]\n","\n","    # Aggregate pattern counts within each group\n","    group1_aggregated = group1.sum(axis=0)\n","    group2_aggregated = group2.sum(axis=0)\n","    # Normalize the aggregated pattern counts to create valid probability distributions\n","    group1_distribution = group1_aggregated / group1_aggregated.sum()\n","    group2_distribution = group2_aggregated / group2_aggregated.sum()\n","\n","    cosine_similarity = 1 - cosine(group1_distribution, group2_distribution)\n","\n","    # Interpretation\n","    if cosine_similarity > 0.95:\n","      r = \"very similar\"\n","    elif cosine_similarity > 0.85:\n","        r = \"similar\"\n","    else:\n","        r = \"not similar\"\n","\n","    results[\"Fold\"].append(i)\n","    results[\"Cosine Similarity\"].append(cosine_similarity)\n","    results[\"Result\"].append(r)\n","\n","  return pd.DataFrame(results)"],"metadata":{"id":"wEKBR0u3IqL3","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":6,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def Cosine_diversity(group1, group2):\n","  group1 = group1.drop(columns=['id', 'class_name'])\n","  group2 = group2.drop(columns=['id', 'class_name'])\n","  # Aggregate pattern counts within each group\n","  group1_aggregated = group1.sum(axis=0)\n","  group2_aggregated = group2.sum(axis=0)\n","\n","  # Normalize the aggregated pattern counts to create valid probability distributions\n","  group1_distribution = group1_aggregated / group1_aggregated.sum()\n","  group2_distribution = group2_aggregated / group2_aggregated.sum()\n","\n","  cosine_similarity = 1 - cosine(group1_distribution, group2_distribution)\n","\n","  # Interpretation\n","  if cosine_similarity > 0.95:\n","      r = \"very similar\"\n","  elif cosine_similarity > 0.85:\n","      r = \"similar\"\n","  else:\n","      r = \"not similar\"\n","\n","  result = {\"Cosine Similarity\": [cosine_similarity], \"Result\": [r]}\n","  return pd.DataFrame(result)\n","\n"],"metadata":{"id":"mgbL41sYKt0o","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":6,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def in_groups(dfs):\n","  for name, df in dfs:\n","    kl_result = KL(df) # one number\n","    ks_result = KS(df) # return df\n","    cosine_result = COSINE(df) # return df\n","    print(f\"KL stability {name}: {kl_result}\")\n","    ks_result.to_csv(f\"{name}_ks_stability.csv\", index=False)\n","    cosine_result.to_csv(f\"{name}_cosine_stability.csv\", index=False)"],"metadata":{"id":"6gGKUCqnZlT4","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":6,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def between_groups(groups):\n","  for name, group1, group2 in groups:\n","    print(name)\n","    kl_result = KL_diversity(group1, group2) # number\n","    ks_result = KS_test(group1, group2)\n","    cosine_result = Cosine_diversity(group1, group2) # return df\n","    print(f\"KL diversity {name}: {kl_result}\")\n","    ks_result.to_csv(f\"{name}_ks_diversity.csv\", index=False)\n","    cosine_result.to_csv(f\"{name}_cosine_diversity.csv\", index=False)"],"metadata":{"id":"2VnIW68JZpb9","executionInfo":{"status":"ok","timestamp":1723889685445,"user_tz":-180,"elapsed":5,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# load data\n","cancer_df = pd.read_csv('/content/gout_cancer_level_4_patterns_2per.csv')\n","cardiac_df = pd.read_csv('/content/gout_cardiac_level_4_patterns_2per.csv')\n","neither_df = pd.read_csv('/content/gout_neither_level_4_patterns_2per.csv')\n","\n","# add label\n","cancer_df['class_name'] = cancer_df['class_name'].replace(True, 'cancer')\n","cardiac_df['class_name'] = cardiac_df['class_name'].replace(True, 'cardiac')\n","neither_df['class_name'] = neither_df['class_name'].replace(True, 'neither')\n","\n","# fill null as 0\n","cancer_df.fillna(0, inplace=True)\n","cardiac_df.fillna(0, inplace=True)\n","neither_df.fillna(0, inplace=True)\n","\n","combined_df = pd.concat([cancer_df, cardiac_df, neither_df], ignore_index=True)\n","combined_df.fillna(0, inplace=True)\n","\n","numeric_cols = combined_df.select_dtypes(include=['number']).columns\n","numeric_cols = list(numeric_cols)[1:] # remove 'id' col\n","\n","# combined_df[numeric_cols] = (combined_df[numeric_cols] - combined_df[numeric_cols].min()) / (combined_df[numeric_cols].max() - combined_df[numeric_cols].min())\n","\n","for col in numeric_cols:\n","    min_col = combined_df[col].min()\n","    max_col = combined_df[col].max()\n","    if max_col - min_col == 0:\n","        combined_df[col] = 0  # or some other appropriate default value\n","    else:\n","        combined_df[col] = (combined_df[col] - min_col) / (max_col - min_col)\n","\n","\n","# Apply Laplace smoothing to numeric columns\n","alpha = 1e-6  # Laplace smoothing factor\n","\n","combined_df[numeric_cols] = combined_df[numeric_cols].applymap(lambda x: x + alpha)\n","\n","# cancer\n","cancer_df = combined_df[combined_df['class_name'] == 'cancer']\n","# cardiac\n","cardiac_df = combined_df[combined_df['class_name'] == 'cardiac']\n","# nither\n","neither_df = combined_df[combined_df['class_name'] == 'neither']\n","\n","dfs = [(\"cancer\", cancer_df), (\"cardiac\", cardiac_df), (\"neither\", neither_df)]\n","in_groups(dfs)\n","\n","\n","# between groups\n","# Cancer vs not cancer\n","not_cancer_df = combined_df[combined_df['class_name'] != 'cancer']\n","# Cardiac vs not Cardiac\n","not_cardiac_df = combined_df[combined_df['class_name'] != 'cardiac']\n","# Neither vs not (cancer or Cardiac)\n","not_neither_df = combined_df[combined_df['class_name'] != 'neither']\n","\n","groups = [(\"cancer\", cancer_df, not_cancer_df), (\"cardiac\", cardiac_df, not_cardiac_df), (\"neither\", neither_df, not_neither_df)]\n","between_groups(groups)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"964QpYxwZvmO","outputId":"9ea550e1-a34b-4ce2-e842-cd2ce556c096","executionInfo":{"status":"ok","timestamp":1723890579994,"user_tz":-180,"elapsed":2473,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-80b69e4c1207>:36: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  combined_df[numeric_cols] = combined_df[numeric_cols].applymap(lambda x: x + alpha)\n"]},{"output_type":"stream","name":"stdout","text":["KL stability cancer: 0.091\n","KL stability cardiac: 0.024\n","KL stability neither: 0.078\n","cancer\n","KL diversity cancer: 5.085901463902687\n","cardiac\n","KL diversity cardiac: 2.280291510878791\n","neither\n","KL diversity neither: 2.9432702960052888\n"]}]},{"cell_type":"markdown","source":["# Local\n"],"metadata":{"id":"rpBmU63JOU8W"}},{"cell_type":"code","source":["for name, group1, group2 in groups:\n","  test_results = []\n","  for column in numeric_cols:\n","      # Summing up the counts for the column in each dataframe\n","      count1 = group1[column].sum()\n","      count2 = group2[column].sum()\n","\n","      # Summing up the total counts (nobs) - using the number of non-null as the observation count\n","      nobs1 = len(cancer_df)\n","      nobs2 = len(not_cancer_df)\n","\n","      # Calculate the Z-test for two proportions\n","      stat, pval = proportions_ztest([count1, count2], [nobs1, nobs2])\n","\n","      # Check if similar or not\n","      similar = 'similar' if pval > 0.05 else 'not similar'\n","\n","      # Append results\n","      test_results.append((column, stat, pval, similar))\n","\n","  # Convert test results to DataFrame\n","  results_df = pd.DataFrame(test_results, columns=['Column', 'Z-Statistic', 'P-Value', 'Similarity'])\n","\n","  # Calculate the percentage of similar columns\n","  not_similar_count = results_df[results_df['Similarity'] == 'not similar'].shape[0]\n","  total_count = results_df.shape[0]\n","  percentage_not_similar = (not_similar_count / total_count) * 100\n","  print(f\"{name}: Similar Percentage {100 - percentage_not_similar}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acJ-ZPjCOdAy","outputId":"b406a076-84bf-468f-b45b-8d17368708bb","executionInfo":{"status":"ok","timestamp":1723890271841,"user_tz":-180,"elapsed":333,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["cancer: Similar Percentage 33.33333333333334\n","cardiac: Similar Percentage 41.880341880341874\n","neither: Similar Percentage 39.31623931623932\n"]}]},{"cell_type":"code","source":["results = {\"name\": [], \"Fold\": [], \"similar\": []}\n","for name, df in dfs:\n","  test_results = []\n","\n","  for i in range(10):\n","    random_indices = np.random.permutation(len(df))\n","    mid_index = len(df) // 2\n","    group1_indices = random_indices[:mid_index]\n","    group2_indices = random_indices[mid_index:]\n","\n","    group1 = df.iloc[group1_indices]\n","    group2 = df.iloc[group2_indices]\n","\n","    for column in numeric_cols:\n","        # Summing up the counts for the column in each dataframe\n","        count1 = group1[column].sum()\n","        count2 = group2[column].sum()\n","\n","        # Summing up the total counts (nobs) - using the number of non-null as the observation count\n","        nobs1 = len(cancer_df)\n","        nobs2 = len(not_cancer_df)\n","\n","        # Calculate the Z-test for two proportions\n","        stat, pval = proportions_ztest([count1, count2], [nobs1, nobs2])\n","\n","        # Check if similar or not\n","        similar = 'similar' if pval > 0.05 else 'not similar'\n","\n","        # Append results\n","        test_results.append((column, stat, pval, similar))\n","\n","    # Convert test results to DataFrame\n","    results_df = pd.DataFrame(test_results, columns=['Column', 'Z-Statistic', 'P-Value', 'Similarity'])\n","\n","    # Calculate the percentage of similar columns\n","    similar_count = results_df[results_df['Similarity'] == 'similar'].shape[0]\n","    total_count = results_df.shape[0]\n","    similar = (similar_count / total_count) * 100\n","    results['name'].append(name)\n","    results['Fold'].append(i)\n","    results['similar'].append(similar)\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"Local_stability.csv\", index=False)"],"metadata":{"id":"douL-uglXLPf","executionInfo":{"status":"ok","timestamp":1723890275361,"user_tz":-180,"elapsed":2057,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# combined_df.to_csv(\"Temporal_Features_Data.csv\", index=False)"],"metadata":{"id":"XfS_GzGS8tKg","executionInfo":{"status":"ok","timestamp":1723889718166,"user_tz":-180,"elapsed":13,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"Ymt00P1sksnE","executionInfo":{"status":"ok","timestamp":1723889718166,"user_tz":-180,"elapsed":12,"user":{"displayName":"Rotem Geller","userId":"01533312932675725826"}}},"execution_count":14,"outputs":[]}]}